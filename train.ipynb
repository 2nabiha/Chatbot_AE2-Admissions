{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TRAINING A CHATBOT INTENT CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "734815fbac8b2b68"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Importing the libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3cd4710065b851"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-20T15:35:44.458251600Z",
     "start_time": "2023-12-20T15:35:23.374034700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dagbo_b40tnyc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Downloading the NLTK packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0b8231bbee48d53"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dagbo_b40tnyc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dagbo_b40tnyc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dagbo_b40tnyc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the wordnet and stopwords corpus\n",
    "nltk.download('wordnet') # wordnet is a lexical database for the English language\n",
    "nltk.download('stopwords') # stopwords means words like 'the', 'a', 'an', 'is', 'are', etc.\n",
    "nltk.download('punkt') # punkt means punctuations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T15:35:44.710961400Z",
     "start_time": "2023-12-20T15:35:44.461257Z"
    }
   },
   "id": "4527ea109c19738d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Importing the intents.json file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2df36a839a3b36fc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# import intents file\n",
    "import json\n",
    "\n",
    "def load_intents():\n",
    "    with open('intents.json') as file:\n",
    "        intents = json.load(file)\n",
    "    return intents\n",
    "\n",
    "intents = load_intents()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T15:35:44.710961400Z",
     "start_time": "2023-12-20T15:35:44.706462800Z"
    }
   },
   "id": "a0903109b6aef2ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. using nltk techniques to preprocess the data for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d37c5c26f7298f46"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# initialize lemmatizer to get stem of words\n",
    "lemmatizer = WordNetLemmatizer() # lemma is the root form of the word and it is very accurate than stemming\n",
    "\n",
    "\n",
    "# loop through each sentence in the intent's patterns\n",
    "def preprocess_intents(intents):\n",
    "    # create empty lists for documents, classes and words\n",
    "    documents = [] # documents means patterns\n",
    "    classes = [] # classes means tags\n",
    "    words = [] # words means vocabulary\n",
    "    ignore_letters = ['!', '?', ',', '.'] # ignore these letters\n",
    "    stop_words = set(stopwords.words('english')) # stop words are words like 'the', 'a', 'an', 'is', 'are', etc.\n",
    "\n",
    "\n",
    "    for intent in intents['intents']:\n",
    "        \n",
    "        # debug for keyerror @ 'patterns'\n",
    "        # print(intent['patterns'])\n",
    "        for pattern in intent['patterns']:\n",
    "            # tokenize each and every word in the sentence\n",
    "            word = nltk.word_tokenize(pattern) \n",
    "            # lemmatize each word and convert into lowercase\n",
    "            word = [lemmatizer.lemmatize(w.lower()) for w in word if w not in stop_words and w not in ignore_letters]\n",
    "            # add word to the word list\n",
    "            words.extend(word) # extend means add to the list and append means add to the end of the list\n",
    "            # add word(s) to documents\n",
    "            documents.append((word, intent['tag'])) #\n",
    "            # add tags to our classes list\n",
    "            if intent['tag'] not in classes: # if tag is not in classes list\n",
    "                classes.append(intent['tag']) # then add it to the classes list\n",
    "                \n",
    "    # sort words and remove duplicates\n",
    "    words = sorted(list(set(words)))\n",
    "    # sort classes\n",
    "    classes = sorted(list(set(classes)))\n",
    "    return documents, classes, words\n",
    "\n",
    "documents, classes, words = preprocess_intents(intents)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T15:35:45.944498100Z",
     "start_time": "2023-12-20T15:35:44.717087700Z"
    }
   },
   "id": "b8545efe3805e21d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Creating training and testing data using bag of words technique or term frequency"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d39823c82283bbc"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232,) (232,) (78,) (78,)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create training and testing data and also convert the words to numbers using bag of words technique\n",
    "def create_training_testing_data(documents, classes, words):\n",
    "    training = [] \n",
    "    # using one hot encoding for our training data with a bag of words\n",
    "    output_empty = [0] * len(classes) # output is a '0' for each tag and '1' for the current tag (for each pattern)\n",
    "    # creating a training set, bag of words for each sentence\n",
    "    for doc in documents:\n",
    "        # initialize a bag of words\n",
    "        bag = []\n",
    "        # list of tokenized words for the pattern\n",
    "        word_patterns = doc[0]\n",
    "        # lemmatize each word - create base word, in an attempt to represent related words\n",
    "        word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "        # create our bag of words arrays with 1 if word matches found in a current pattern\n",
    "        for word in words:\n",
    "            bag.append(1) if word in word_patterns else bag.append(0)\n",
    "            \n",
    "        # output is a '0' for each tag and '1' for the current tag (for each pattern)\n",
    "        output_row = list(output_empty)\n",
    "        output_row[classes.index(doc[1])] = 1\n",
    "        \n",
    "        training.append([bag, output_row])\n",
    "        \n",
    "    # shuffle the features and make numpy array\n",
    "    random.shuffle(training)\n",
    "    training = np.array(training, dtype=object)\n",
    "    \n",
    "    \n",
    "    # Splitting features and target (label) for training and testing using sklearn\n",
    "    train_x, test_x, train_y, test_y = train_test_split(training[:,0], training[:,1], test_size=0.25)\n",
    "    return numpy.array(train_x), numpy.array(train_y), numpy.array(test_x), numpy.array(test_y)\n",
    "\n",
    "# test the function\n",
    "train_x, train_y, test_x, test_y = create_training_testing_data(documents, classes, words)\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T15:35:45.964680500Z",
     "start_time": "2023-12-20T15:35:45.959025500Z"
    }
   },
   "id": "f9403a18f3ccf496"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Building the model with neural networks using mlp classifier and also lstm\n",
    "## comparing the performance of both models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e705aa3dce9bc62"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 47\u001B[0m\n\u001B[0;32m     43\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[1;32m---> 47\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwords\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclasses\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[14], line 19\u001B[0m, in \u001B[0;36mcreate_model\u001B[1;34m(train_x, train_y, test_x, test_y, words, classes)\u001B[0m\n\u001B[0;32m     16\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39msgd, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# fitting and saving the model\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m hist \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_x\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_y\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchatbot_model.h5\u001B[39m\u001B[38;5;124m'\u001B[39m, hist)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# evaluate model performance\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[1;34m(value, ctx, dtype)\u001B[0m\n\u001B[0;32m    101\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[0;32m    102\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m--> 103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# set up the model for lstm\n",
    "def create_model(train_x, train_y, test_x, test_y, words, classes):\n",
    "    # create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "    # equal to number of intents to predict output intent with softmax\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(Dense(32, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "    \n",
    "    # Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "    sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True) # stochastic gradient descent\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    # fitting and saving the model\n",
    "    hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "    model.save('chatbot_model.h5', hist)\n",
    "    \n",
    "    # evaluate model performance\n",
    "    val_loss, val_acc = model.evaluate(np.array(test_x), np.array(test_y), verbose=0)\n",
    "    print(val_loss, val_acc)\n",
    "    \n",
    "    # roc auc score and curve\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    # predict probabilities\n",
    "    probs = model.predict_proba(np.array(test_x))\n",
    "    # keep probabilities for the positive outcome only\n",
    "    probs = probs[:, 1]\n",
    "    # calculate roc auc\n",
    "    auc = roc_auc_score(np.array(test_y), probs)\n",
    "    print('ROC AUC=%.3f' % auc)\n",
    "    # calculate roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(np.array(test_y), probs)\n",
    "    # plot no skill\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(train_x, train_y, test_x, test_y, words, classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T15:44:56.535331200Z",
     "start_time": "2023-12-20T15:44:56.430614200Z"
    }
   },
   "id": "396acba8d63a41a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Loading the model and testing it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84c8ae9ebf2725a7"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at chatbot_model.h5",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# load the model\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_model\n\u001B[1;32m----> 3\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mchatbot_model.h5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:262\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001B[0m\n\u001B[0;32m    254\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m saving_lib\u001B[38;5;241m.\u001B[39mload_model(\n\u001B[0;32m    255\u001B[0m         filepath,\n\u001B[0;32m    256\u001B[0m         custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects,\n\u001B[0;32m    257\u001B[0m         \u001B[38;5;28mcompile\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcompile\u001B[39m,\n\u001B[0;32m    258\u001B[0m         safe_mode\u001B[38;5;241m=\u001B[39msafe_mode,\n\u001B[0;32m    259\u001B[0m     )\n\u001B[0;32m    261\u001B[0m \u001B[38;5;66;03m# Legacy case.\u001B[39;00m\n\u001B[1;32m--> 262\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlegacy_sm_saving_lib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    263\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_objects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_objects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcompile\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mcompile\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    264\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\legacy\\save.py:234\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(filepath, custom_objects, compile, options)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(filepath_str, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mio\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39mexists(filepath_str):\n\u001B[1;32m--> 234\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(\n\u001B[0;32m    235\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo file or directory found at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    236\u001B[0m         )\n\u001B[0;32m    238\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mio\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39misdir(filepath_str):\n\u001B[0;32m    239\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m saved_model_load\u001B[38;5;241m.\u001B[39mload(\n\u001B[0;32m    240\u001B[0m             filepath_str, \u001B[38;5;28mcompile\u001B[39m, options\n\u001B[0;32m    241\u001B[0m         )\n",
      "\u001B[1;31mOSError\u001B[0m: No file or directory found at chatbot_model.h5"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('chatbot_model.h5')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T15:44:37.081248200Z",
     "start_time": "2023-12-20T15:44:37.002227Z"
    }
   },
   "id": "9ae48f06d2e96d2a"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9508beb88d7618cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
