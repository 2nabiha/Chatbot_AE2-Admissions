{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TRAINING A CHATBOT INTENT CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "734815fbac8b2b68"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Importing the libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3cd4710065b851"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:28:15.854427900Z",
     "start_time": "2024-01-04T16:28:15.719326300Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'inspect' has no attribute 'formatargspec'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\__init__.py:115\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;66;03m###########################################################\u001B[39;00m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;66;03m# TOP-LEVEL MODULES\u001B[39;00m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;66;03m###########################################################\u001B[39;00m\n\u001B[0;32m    111\u001B[0m \n\u001B[0;32m    112\u001B[0m \u001B[38;5;66;03m# Import top-level functionality into top-level namespace\u001B[39;00m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcollocations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m--> 115\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecorators\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m decorator, memoize\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeatstruct\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgrammar\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:187\u001B[0m\n\u001B[0;32m    184\u001B[0m         \u001B[38;5;28msetattr\u001B[39m(obj, name, default)\n\u001B[0;32m    185\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m default\n\u001B[1;32m--> 187\u001B[0m \u001B[38;5;129;43m@decorator\u001B[39;49m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43mmemoize\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdic\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mgetattr_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemoize_dic\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# memoize_dic is created at the first call\u001B[39;49;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:176\u001B[0m, in \u001B[0;36mdecorator\u001B[1;34m(caller)\u001B[0m\n\u001B[0;32m    174\u001B[0m     dec_func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28meval\u001B[39m(src, \u001B[38;5;28mdict\u001B[39m(_func_\u001B[38;5;241m=\u001B[39mfunc, _call_\u001B[38;5;241m=\u001B[39mcaller))\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m update_wrapper(dec_func, func, infodict)\n\u001B[1;32m--> 176\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mupdate_wrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_decorator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaller\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:87\u001B[0m, in \u001B[0;36mupdate_wrapper\u001B[1;34m(wrapper, model, infodict)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate_wrapper\u001B[39m(wrapper, model, infodict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m---> 87\u001B[0m     infodict \u001B[38;5;241m=\u001B[39m infodict \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mgetinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     88\u001B[0m     wrapper\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m=\u001B[39m infodict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     89\u001B[0m     wrapper\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__doc__\u001B[39m \u001B[38;5;241m=\u001B[39m infodict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdoc\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:69\u001B[0m, in \u001B[0;36mgetinfo\u001B[1;34m(func)\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m varkwargs:\n\u001B[0;32m     68\u001B[0m     argnames\u001B[38;5;241m.\u001B[39mappend(varkwargs)\n\u001B[1;32m---> 69\u001B[0m signature \u001B[38;5;241m=\u001B[39m \u001B[43minspect\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformatargspec\u001B[49m(regargs, varargs, varkwargs, defaults,\n\u001B[0;32m     70\u001B[0m                                   formatvalue\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m value: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     72\u001B[0m \u001B[38;5;66;03m# pypy compatibility\u001B[39;00m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(func, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__closure__\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'inspect' has no attribute 'formatargspec'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Downloading the NLTK packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0b8231bbee48d53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download the wordnet and stopwords corpus\n",
    "nltk.download('wordnet') # wordnet is a lexical database for the English language\n",
    "nltk.download('stopwords') # stopwords means words like 'the', 'a', 'an', 'is', 'are', etc.\n",
    "nltk.download('punkt') # punkt means punctuations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-04T16:25:54.216934300Z"
    }
   },
   "id": "4527ea109c19738d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Importing the intents.json file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2df36a839a3b36fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import intents file\n",
    "import json\n",
    "\n",
    "def load_intents():\n",
    "    with open('intents.json') as file:\n",
    "        intents = json.load(file)\n",
    "    return intents\n",
    "\n",
    "intents = load_intents()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-04T16:25:54.218936800Z"
    }
   },
   "id": "a0903109b6aef2ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. using nltk techniques to preprocess the data for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d37c5c26f7298f46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize lemmatizer to get stem of words\n",
    "lemmatizer = WordNetLemmatizer() # lemma is the root form of the word and it is very accurate than stemming\n",
    "\n",
    "\n",
    "# loop through each sentence in the intent's patterns\n",
    "def preprocess_intents(intents):\n",
    "    # create empty lists for documents, classes and words\n",
    "    documents = [] # documents means patterns\n",
    "    classes = [] # classes means tags\n",
    "    words = [] # words means vocabulary\n",
    "    ignore_letters = ['!', '?', ',', '.'] # ignore these letters\n",
    "    stop_words = set(stopwords.words('english')) # stop words are words like 'the', 'a', 'an', 'is', 'are', etc.\n",
    "\n",
    "\n",
    "    for intent in intents['intents']:\n",
    "        \n",
    "        # debug for keyerror @ 'patterns'\n",
    "        # print(intent['patterns'])\n",
    "        for pattern in intent['patterns']:\n",
    "            # tokenize each and every word in the sentence\n",
    "            word = nltk.word_tokenize(pattern) \n",
    "            # lemmatize each word and convert into lowercase\n",
    "            word = [lemmatizer.lemmatize(w.lower()) for w in word if w not in stop_words and w not in ignore_letters]\n",
    "            # add word to the word list\n",
    "            words.extend(word) # extend means add to the list and append means add to the end of the list\n",
    "            # add word(s) to documents\n",
    "            documents.append((word, intent['tag'])) #\n",
    "            # add tags to our classes list\n",
    "            if intent['tag'] not in classes: # if tag is not in classes list\n",
    "                classes.append(intent['tag']) # then add it to the classes list\n",
    "                \n",
    "    # sort words and remove duplicates\n",
    "    words = sorted(list(set(words)))\n",
    "    # sort classes\n",
    "    classes = sorted(list(set(classes)))\n",
    "    return documents, classes, words\n",
    "\n",
    "documents, classes, words = preprocess_intents(intents)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:25:54.241836300Z",
     "start_time": "2024-01-04T16:25:54.221447900Z"
    }
   },
   "id": "b8545efe3805e21d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Creating training and testing data using bag of words technique or term frequency"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d39823c82283bbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create training and testing data and also convert the words to numbers using bag of words technique\n",
    "def create_training_testing_data(documents, classes, words):\n",
    "    training = [] \n",
    "    # using one hot encoding for our training data with a bag of words\n",
    "    output_empty = [0] * len(classes) # output is a '0' for each tag and '1' for the current tag (for each pattern)\n",
    "    # creating a training set, bag of words for each sentence\n",
    "    for doc in documents:\n",
    "        # initialize a bag of words\n",
    "        bag = []\n",
    "        # list of tokenized words for the pattern\n",
    "        word_patterns = doc[0]\n",
    "        # lemmatize each word - create base word, in an attempt to represent related words\n",
    "        word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "        # create our bag of words arrays with 1 if word matches found in a current pattern\n",
    "        for word in words:\n",
    "            bag.append(1) if word in word_patterns else bag.append(0)\n",
    "            \n",
    "        # output is a '0' for each tag and '1' for the current tag (for each pattern)\n",
    "        output_row = list(output_empty)\n",
    "        output_row[classes.index(doc[1])] = 1\n",
    "        \n",
    "        training.append([bag, output_row])\n",
    "        \n",
    "    # shuffle the features and make numpy array\n",
    "    random.shuffle(training)\n",
    "    training = np.array(training, dtype=object)\n",
    "    \n",
    "    \n",
    "    # Splitting features and target (label) for training and testing using sklearn\n",
    "    train_x, test_x, train_y, test_y = train_test_split(training[:,0], training[:,1], test_size=0.25)\n",
    "    return numpy.array(train_x), numpy.array(train_y), numpy.array(test_x), numpy.array(test_y)\n",
    "\n",
    "# test the function\n",
    "train_x, train_y, test_x, test_y = create_training_testing_data(documents, classes, words)\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-04T16:25:54.223447500Z"
    }
   },
   "id": "f9403a18f3ccf496"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Building the model with neural networks using mlp classifier and also lstm\n",
    "## comparing the performance of both models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e705aa3dce9bc62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set up the model for lstm\n",
    "def create_model(train_x, train_y, test_x, test_y, words, classes):\n",
    "    # create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "    # equal to number of intents to predict output intent with softmax\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(Dense(32, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "    \n",
    "    # Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "    sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True) # stochastic gradient descent\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    # fitting and saving the model\n",
    "    hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "    model.save('chatbot_model.h5', hist)\n",
    "    \n",
    "    # evaluate model performance\n",
    "    val_loss, val_acc = model.evaluate(np.array(test_x), np.array(test_y), verbose=0)\n",
    "    print(val_loss, val_acc)\n",
    "    \n",
    "    # roc auc score and curve\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    # predict probabilities\n",
    "    probs = model.predict_proba(np.array(test_x))\n",
    "    # keep probabilities for the positive outcome only\n",
    "    probs = probs[:, 1]\n",
    "    # calculate roc auc\n",
    "    auc = roc_auc_score(np.array(test_y), probs)\n",
    "    print('ROC AUC=%.3f' % auc)\n",
    "    # calculate roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(np.array(test_y), probs)\n",
    "    # plot no skill\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(train_x, train_y, test_x, test_y, words, classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-04T16:25:54.225447200Z"
    }
   },
   "id": "396acba8d63a41a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Loading the model and testing it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84c8ae9ebf2725a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('chatbot_model.h5')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-04T16:25:54.227446500Z"
    }
   },
   "id": "9ae48f06d2e96d2a"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9508beb88d7618cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
